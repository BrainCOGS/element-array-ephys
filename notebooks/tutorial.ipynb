{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataJoint Elements for Array Electrophysiology with NeuroPixels\n",
    "\n",
    "#### Open-source data pipeline for processing and analyzing extracellular electrophysiology datasets.\n",
    "\n",
    "Welcome to the tutorial for the DataJoint Element for extracellular array electrophysiology. This\n",
    "tutorial aims to provide a comprehensive understanding of the open-source data pipeline\n",
    "created using `element-array-ephys`.\n",
    "\n",
    "This package is designed to seamlessly process, ingest, and track extracellular electrophysiology\n",
    "data, along with its associated probe and recording metadata. By the end of this\n",
    "tutorial you will have a clear grasp on setting up and integrating `element-array-ephys`\n",
    "into your specific research projects and lab. \n",
    "\n",
    "![flowchart](../images/diagram_flowchart.svg)\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "Please see the [datajoint tutorials GitHub\n",
    "repository](https://github.com/datajoint/datajoint-tutorials/tree/main) before\n",
    "proceeding.\n",
    "\n",
    "A basic understanding of the following DataJoint concepts will be beneficial to your\n",
    "understanding of this tutorial: \n",
    "1. The `Imported` and `Computed` tables types in `datajoint-python`.\n",
    "2. The functionality of the `.populate()` method. \n",
    "\n",
    "#### **Tutorial Overview**\n",
    "\n",
    "+ Setup\n",
    "+ *Activate* the DataJoint pipeline.\n",
    "+ *Insert* subject, session, and probe metadata.\n",
    "+ *Populate* electrophysiology recording metadata.\n",
    "+ Run the clustering task.\n",
    "+ Curate the results (optional).\n",
    "+ Visualize the results.\n",
    "\n",
    "### **Setup**\n",
    "\n",
    "This tutorial examines extracellular electrophysiology data acquired with `OpenEphys`\n",
    "and spike-sorted using Kilosort 2.5. The goal is to store, track\n",
    "and manage sessions of array electrophysiology data, including spike sorting results and\n",
    "unit-level visualizations. \n",
    "\n",
    "The results of this Element can be combined with **other modalities** to create\n",
    "a complete, customizable data pipeline for your specific lab or study. For instance, you\n",
    "can combine `element-array-ephys` with `element-calcium-imaging` and\n",
    "`element-deeplabcut` to characterize the neural activity along with markless\n",
    "pose-estimation during behavior.\n",
    "\n",
    "Let's start this tutorial by importing the packages necessary to run the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datajoint as dj\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the tutorial is run in Codespaces, a private, local database server is created and\n",
    "made available for you. This is where we will insert and store our processed results.\n",
    "Let's connect to the database server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dj.conn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Activate the DataJoint Pipeline**\n",
    "\n",
    "This tutorial activates the `ephys_acute.py` module from `element-array-ephys`, along\n",
    "with upstream dependencies from `element-animal` and `element-session`. Please refer to the\n",
    "[`tutorial_pipeline.py`](./tutorial_pipeline.py) for the source code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tutorial_pipeline import lab, subject, session, probe, ephys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can represent the tables in the `probe` and `ephys` schemas as well as some of the\n",
    "upstream dependencies to `session` and `subject` schemas as a diagram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    dj.Diagram(subject.Subject)\n",
    "    + dj.Diagram(session.Session)\n",
    "    + dj.Diagram(probe)\n",
    "    + dj.Diagram(ephys)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As evident from the diagram, this data pipeline encompasses tables associated with\n",
    "recording and probe metadata, results of clustering, and optional curation of clustering\n",
    "results. A few tables, such as `subject.Subject` or `session.Session`,\n",
    "while important for a complete pipeline, fall outside the scope of the `element-array-ephys`\n",
    "tutorial, and will therefore, not be explored extensively here. The primary focus of\n",
    "this tutorial will be on the `probe` and `ephys` schemas.\n",
    "\n",
    "### **Insert subject, session, and probe metadata**\n",
    "\n",
    "Let's start with the first table in the schema diagram (i.e. `subject.Subject` table).\n",
    "\n",
    "To know what data to insert into the table, we can view its dependencies and attributes using the `.describe()` and `.heading` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject.Subject()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(subject.Subject.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject.Subject.heading"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells above show all attributes of the subject table.\n",
    "We will insert data into the\n",
    "`subject.Subject` table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject.Subject.insert1(\n",
    "    dict(subject=\"subject5\", subject_birth_date=\"2023-01-01\", sex=\"U\")\n",
    ")\n",
    "subject.Subject()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's repeat the steps above for the `Session` table and see how the output varies between\n",
    "`.describe` and `.heading`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(session.Session.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.Session.heading"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that `describe`, displays the table's structure and highlights its dependencies, such as its reliance on the `Subject` table. These dependencies represent foreign key references, linking data across tables.\n",
    "\n",
    "On the other hand, `heading` provides an exhaustive list of the table's attributes. This\n",
    "list includes both the attributes declared in this table and any inherited from upstream\n",
    "tables.\n",
    "\n",
    "With this understanding, let's move on to insert a session associated with our subject.\n",
    "\n",
    "We will insert into the `session.Session` table by passing a dictionary to the `insert1` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_key = dict(subject=\"subject5\", session_datetime=\"2023-01-01 00:00:00\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.Session.insert1(session_key)\n",
    "session.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every experimental session produces a set of data files. The purpose of the `SessionDirectory` table is to locate these files. It references a directory path relative to a root directory, defined in `dj.config[\"custom\"]`. More information about `dj.config` is provided in the [documentation](https://datajoint.com/docs/elements/user-guide/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.SessionDirectory.insert1(\n",
    "    dict(**session_key, session_dir=\"raw/subject5/session1\")\n",
    ")\n",
    "session.SessionDirectory()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the Diagram indicates, the tables in the `probe` schemas need to\n",
    "contain data before the tables in the `ephys` schema accept any data. Let's\n",
    "start by inserting into `probe.Probe`, a table containing metadata about a\n",
    "multielectrode probe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe.Probe.insert1(\n",
    "    dict(probe=\"714000838\", probe_type=\"neuropixels 1.0 - 3B\")\n",
    ")  # this info could be achieve from neuropixels meta file.\n",
    "probe.Probe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probe metadata is used by the downstream `ProbeInsertion` table which we\n",
    "insert data into in the cells below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ephys.ProbeInsertion.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys.ProbeInsertion.heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys.ProbeInsertion.insert1(\n",
    "    dict(\n",
    "        session_key,\n",
    "        insertion_number=1,\n",
    "        probe=\"714000838\",\n",
    "    )\n",
    ")  # probe, subject, session_datetime needs to follow the restrictions of foreign keys.\n",
    "ephys.ProbeInsertion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm the inserted data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys.ProbeInsertion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Populate electrophysiology recording metadata**\n",
    "\n",
    "In the upcoming cells, the `.populate()` method will automatically extract and store the\n",
    "recording metadata for each experimental session in the `ephys.EphysRecording` table and its part table `ephys.EphysRecording.EphysFile`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys.EphysRecording()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys.EphysRecording.EphysFile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys.EphysRecording.populate(session_key, display_progress=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's view the information was entered into each of these tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys.EphysRecording()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys.EphysRecording.EphysFile()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Run the Clustering Task**\n",
    "\n",
    "We're almost ready to spike sort the data with `kilosort`. An important step before\n",
    "processing is managing the parameters which will be used in that step. To do so, we will\n",
    "define the kilosort parameters in a dictionary and insert them into a DataJoint table\n",
    "`ClusteringParamSet`. This table keeps track of all combinations of your spike sorting\n",
    "parameters. You can choose which parameters are used during processing in a later step.\n",
    "\n",
    "Let's view the attributes and insert data into `ephys.ClusteringParamSet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys.ClusteringParamSet.heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert clustering task manually\n",
    "params_ks = {\n",
    "    \"fs\": 30000,\n",
    "    \"fshigh\": 150,\n",
    "    \"minfr_goodchannels\": 0.1,\n",
    "    \"Th\": [10, 4],\n",
    "    \"lam\": 10,\n",
    "    \"AUCsplit\": 0.9,\n",
    "    \"minFR\": 0.02,\n",
    "    \"momentum\": [20, 400],\n",
    "    \"sigmaMask\": 30,\n",
    "    \"ThPr\": 8,\n",
    "    \"spkTh\": -6,\n",
    "    \"reorder\": 1,\n",
    "    \"nskip\": 25,\n",
    "    \"GPU\": 1,\n",
    "    \"Nfilt\": 1024,\n",
    "    \"nfilt_factor\": 4,\n",
    "    \"ntbuff\": 64,\n",
    "    \"whiteningRange\": 32,\n",
    "    \"nSkipCov\": 25,\n",
    "    \"scaleproc\": 200,\n",
    "    \"nPCs\": 3,\n",
    "    \"useRAM\": 0,\n",
    "}\n",
    "ephys.ClusteringParamSet.insert_new_params(\n",
    "    clustering_method=\"kilosort2\",\n",
    "    paramset_idx=0,\n",
    "    params=params_ks,\n",
    "    paramset_desc=\"Spike sorting using Kilosort2\",\n",
    ")\n",
    "ephys.ClusteringParamSet()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataJoint uses a `ClusteringTask` table to\n",
    "manage which `EphysRecording` and `ClusteringParamSet` should be used during processing. \n",
    "\n",
    "This table is important for defining several important aspects of\n",
    "downstream processing. Let's view the attributes to get a better understanding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys.ClusteringTask.heading"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ClusteringTask` table contains two important attributes: \n",
    "+ `paramset_idx` - Allows the user to choose the parameter set with which you want to\n",
    "  run spike sorting.\n",
    "+ `task_mode` - Can be set to `load` or `trigger`. When set to `load`, running the\n",
    "  Clustering step initiates a search for existing output files of the spike sorting\n",
    "  algorithm defined in `ClusteringParamSet`. When set to `trigger`, the processing step\n",
    "  will run spike sorting on the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys.ClusteringTask.insert1(\n",
    "    dict(\n",
    "        session_key,\n",
    "        insertion_number=1,\n",
    "        paramset_idx=0,\n",
    "        task_mode=\"load\",  # load or trigger\n",
    "        clustering_output_dir=\"processed/subject5/session1/probe_1/kilosort2-5_1\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's call populate on the `Clustering` table which checks for kilosort results since `task_mode=load`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys.Clustering.populate(session_key, display_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Curate the results (Optional)**\n",
    "\n",
    "While spike sorting is completed in the above step, you can optionally curate\n",
    "the output of image processing using the `Curation` table. For this demo, we\n",
    "will simply use the results of the spike sorting output from the `Clustering` task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys.Curation.heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_key = (ephys.ClusteringTask & session_key).fetch1(\"KEY\")\n",
    "ephys.Curation().create1_from_clustering_task(clustering_key)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the `Curation` table receives an entry, we can populate the remaining\n",
    "tables in the workflow including `CuratedClustering`, `WaveformSet`, and `LFP`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys.CuratedClustering.populate(session_key, display_progress=True)\n",
    "ephys.LFP.populate(session_key, display_progress=True)\n",
    "ephys.WaveformSet.populate(session_key, display_progress=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've populated the tables in this DataJoint pipeline, there are one of\n",
    "several next steps. If you have an existing pipeline for\n",
    "aligning waveforms to behavior data or other stimuli, you can easily\n",
    "invoke `element-event` or define your custom DataJoint tables to extend the\n",
    "pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Visualize the results**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will do some exploratory analysis by fetching the data from the database and creating a few plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfp_average = (ephys.LFP & \"insertion_number = '1'\").fetch1(\"lfp_mean\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the query above, we fetch a single `lfp_mean` attribute from the `LFP` table.\n",
    "We also restrict the query to insertion number 1.\n",
    "\n",
    "Let's go ahead and plot the LFP mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lfp_average)\n",
    "plt.title(\"Average LFP Waveform for Insertion 1\")\n",
    "plt.xlabel(\"Samples\")\n",
    "plt.ylabel(\"microvolts (uV)\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataJoint queries are a highly flexible tool to manipulate and visualize your data.\n",
    "After all, visualizing traces or generating rasters is likely just the start of\n",
    "your analysis workflow. This can also make the queries seem more complex at\n",
    "first. However, we'll walk through them slowly to simplify their content in this notebook. \n",
    "\n",
    "The examples below perform several operations using DataJoint queries:\n",
    "- Fetch the primary key attributes of all units that are in `insertion_number=1`.\n",
    "- Use **multiple restrictions** to fetch timestamps and create a raster plot.\n",
    "- Use a **join** operation and **multiple restrictions** to fetch a waveform\n",
    "  trace, along with unit data to create a single waveform plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_key = (ephys.ProbeInsertion & \"insertion_number = '1'\").fetch1(\"KEY\")\n",
    "units, unit_spiketimes = (\n",
    "    ephys.CuratedClustering.Unit\n",
    "    & insert_key\n",
    "    & 'unit IN (\"6\",\"7\",\"9\",\"14\",\"15\",\"17\",\"19\")'\n",
    ").fetch(\"unit\", \"spike_times\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.hstack(unit_spiketimes)\n",
    "y = np.hstack([np.full_like(s, u) for u, s in zip(units, unit_spiketimes)])\n",
    "plt.plot(x, y, \"|\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Unit\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we will use two queries to fetch *all* of the information about a single unit and\n",
    "plot the unit waveform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_key = (ephys.CuratedClustering.Unit & insert_key & \"unit = '15'\").fetch1(\"KEY\")\n",
    "unit_data = (\n",
    "    ephys.CuratedClustering.Unit * ephys.WaveformSet.PeakWaveform & unit_key\n",
    ").fetch1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_rate = (ephys.EphysRecording & insert_key).fetch1(\n",
    "    \"sampling_rate\"\n",
    ") / 1000  # in kHz\n",
    "plt.plot(\n",
    "    np.r_[: unit_data[\"peak_electrode_waveform\"].size] * 1 / sampling_rate,\n",
    "    unit_data[\"peak_electrode_waveform\"],\n",
    ")\n",
    "plt.xlabel(\"Time (ms)\")\n",
    "plt.ylabel(r\"Voltage ($\\mu$V)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Following this tutorial, we have: \n",
    "+ Covered the essential functionality of `element-array-ephys`.\n",
    "+ Learned how to manually insert data into tables.\n",
    "+ Executed and ingested results of spike sorting with Kilosort.\n",
    "+ Visualized the results. \n",
    "\n",
    "#### Documentation and DataJoint Tutorials\n",
    "\n",
    "+ [Detailed documentation on\n",
    "  `element-array-ephys`.](https://datajoint.com/docs/elements/element-array-ephys/)\n",
    "+ [General `datajoint-python`\n",
    "  tutorials.](https://github.com/datajoint/datajoint-tutorials) covering fundamentals,\n",
    "  such as table tiers, query operations, fetch operations, automated computations with the\n",
    "  make function, and more.\n",
    "+ [Documentation for\n",
    "  `datajoint-python`.](https://datajoint.com/docs/core/datajoint-python/)\n",
    "\n",
    "##### Run this tutorial on your own data\n",
    "\n",
    "To run this tutorial notebook on your own data, please use the following steps:\n",
    "+ Download the [mysql-docker image for\n",
    "  DataJoint](https://github.com/datajoint/mysql-docker) and run the container according\n",
    "  to the instructions provide in the repository.\n",
    "+ Create a fork of this repository to your GitHub account.\n",
    "+ Clone the repository and open the files using your IDE.\n",
    "+ Add a code cell immediately after the first code cell in the notebook - we will setup\n",
    "  the local connection using this cell. In this cell, type in the following code. \n",
    "\n",
    "```python\n",
    "import datajoint as dj\n",
    "dj.config[\"database.host\"] = \"localhost\"\n",
    "dj.config[\"database.user\"] = \"<your-username>\"\n",
    "dj.config[\"database.password\"] = \"<your-password>\"\n",
    "dj.config[\"custom\"] = {\"imaging_root_data_dir\": \"path/to/your/data/dir\",\n",
    "\"database_prefix\": \"<your-username_>\"}\n",
    "dj.config.save_local()\n",
    "dj.conn()\n",
    "```\n",
    "\n",
    "+ Run the code block above and proceed with the rest of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3p10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "vscode": {
   "interpreter": {
    "hash": "ff52d424e56dd643d8b2ec122f40a2e279e94970100b4e6430cb9025a65ba4cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
