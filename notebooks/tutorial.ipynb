{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataJoint Elements for Array Electrophysiology with NeuroPixels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Open-source Data Pipeline for Processing and Analyzing Extracellular Electrophysiology Datasets**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial aims to provide a comprehensive understanding of the open-source data pipeline by `element-array-ephys`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![flowchart](../images/diagram_flowchart.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The package is designed to **process NeuroPixels ephys data** with **OpenEphys** and spike sorted with **Kilosort**. The following Diagram corresponds to the `ephys_acute` module:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pipeline](../images/attached_array_ephys_element_acute.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the end of this tutorial, you will have a clear grasp of how to set up and integrate the `element-array-ephys` into your specific research projects and your lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Key Components and Objectives**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**- Setup**\n",
    "\n",
    "**- Designing the DataJoint Pipeline**\n",
    "\n",
    "**- Step 1: Insert Example Data into Subject and Session tables**\n",
    "\n",
    "**- Step 2: Register the Electrophysiology Recording information for each Probe**\n",
    "\n",
    "**- Step 3: Run the Clustering Task**\n",
    "\n",
    "**- Step 4: Curate the Clustering Results (Optional)**\n",
    "\n",
    "**- Step 5: Visualize the Results**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial examines the `ephys_acute` module applied to physiological recordings and automatic ingestion of spike sorting results.\n",
    "\n",
    "The goal is to store, track and manage different curations of the spike sorting results and unit-level visualization results.\n",
    "\n",
    "The results of this Element example can be combined with other modalities to create a complete customizable data pipeline for your specific lab or study. For instance, you can combine `element-array-ephys` with `element-calcium-imaging` and `element-deeplabcut` to characterize the neural activity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start this tutorial by importing the packages necessary to run the data pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datajoint as dj\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This codespace provides a local database private to you for experimentation. Let's connect to the database server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dj.conn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Design the DataJoint Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial presumes that the `element-array-ephys` has been pre-configured and instantiated, with the database linked downstream to pre-existing `subject` and `session` tables. \n",
    "\n",
    "Now, we will proceed to import the essential schemas required to construct this data pipeline, with particular attention to the primary components: `probe` and `ephys`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tutorial_pipeline import lab, subject, session, probe, ephys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can represent a diagram of some of the upstream and downstream dependencies connected to these `probe` and `ephys` schemas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    dj.Diagram(subject.Subject)\n",
    "    + dj.Diagram(session.Session)\n",
    "    + dj.Diagram(probe)\n",
    "    + dj.Diagram(ephys)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As evident, this data pipeline is fairly comprehensive, encompassing several tables associated with different Array Electrophysiology components like ephys recording, probe, and clustering. A few tables, such as `Subject` or `Session`, while integral to the pipeline, fall outside the scope of `element-array-ephys` tutorial as they are upstream. \n",
    "\n",
    "Our focus in this tutorial will be primarily on the two core schemas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dj.Diagram(probe) + dj.Diagram(ephys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This diagram represents an example of the `element-array-ephys` pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 1 - Insert Example Data into Subject and Session tables**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the first table in the schema diagram (i.e. `subject.Subject` table).\n",
    "\n",
    "To know what data to insert into the table, we can view its dependencies and attributes using the `.describe()` and `.heading` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject.Subject()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(subject.Subject.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject.Subject.heading"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells above show all attributes of the subject table.\n",
    "We will insert data into the\n",
    "`subject.Subject` table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject.Subject.insert1(\n",
    "    dict(subject=\"subject5\", subject_birth_date=\"2023-01-01\", sex=\"U\")\n",
    ")\n",
    "subject.Subject()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's repeat the steps above for the `Session` table and see how the output varies between\n",
    "`.describe` and `.heading`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(session.Session.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.Session.heading"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that `describe`, displays the table's structure and highlights its dependencies, such as its reliance on the `Subject` table. These dependencies represent foreign key references, linking data across tables.\n",
    "\n",
    "On the other hand, `heading` provides an exhaustive list of the table's attributes. This\n",
    "list includes both the attributes declared in this table and any inherited from upstream\n",
    "tables.\n",
    "\n",
    "With this understanding, let's move on to insert a session associated with our subject.\n",
    "\n",
    "We will insert into the `session.Session` table by passing a dictionary to the `insert1` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_key = dict(subject=\"subject5\", session_datetime=\"2023-01-01 00:00:00\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.Session.insert1(session_key)\n",
    "session.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 2: Register the Electrophysiology Recording information for each Probe**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every experimental session produces a set of data files. The purpose of the `SessionDirectory` table is to locate these files. It references a directory path relative to a root directory, defined in `dj.config[\\\"custom\\\"]`. More information about `dj.config` is provided in the [Documentation](https://datajoint.com/docs/elements/user-guide/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.SessionDirectory.insert1(\n",
    "    dict(**session_key, session_dir=\"raw/subject5/session1\")\n",
    ")\n",
    "session.SessionDirectory()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the Diagram indicates, the tables in the `probe` schemas need to\n",
    "contain data before the tables in the `ephys` schema accept any data. Let's\n",
    "start by inserting into `probe.Probe`, a table containing metadata about a\n",
    "multielectrode probe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe.Probe.insert1(\n",
    "    dict(probe=\"714000838\", probe_type=\"neuropixels 1.0 - 3B\")\n",
    ")  # this info could be achieve from neuropixels meta file.\n",
    "probe.Probe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probe metadata is used by the downstream `ProbeInsertion` table which we\n",
    "insert data into in the cells below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ephys.ProbeInsertion.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys.ProbeInsertion.heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys.ProbeInsertion.insert1(\n",
    "    dict(\n",
    "        session_key,\n",
    "        insertion_number=1,\n",
    "        probe=\"714000838\",\n",
    "    )\n",
    ")  # probe, subject, session_datetime needs to follow the restrictions of foreign keys.\n",
    "ephys.ProbeInsertion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm the inserted data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys.ProbeInsertion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the upcoming cells, populate the `ephys.EphysRecording` table and its part table `ephys.EphysRecording.EphysFile` will extract and store the recording information from a given experimental session.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys.EphysRecording()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys.EphysRecording.EphysFile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys.EphysRecording.populate(session_key, display_progress=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's view the information was entered into each of these tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys.EphysRecording()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys.EphysRecording.EphysFile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 3: Run the Clustering Task**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're almost ready to spike sort the data with `kilosort`. An important step before\n",
    "processing is managing the parameters which will be used in that step. To do so, we will\n",
    "define the kilosort parameters in a dictionary and insert them into a DataJoint table\n",
    "`ClusteringParamSet`. This table keeps track of all combinations of your spike sorting\n",
    "parameters. You can choose which parameters are used during processing in a later step.\n",
    "\n",
    "Let's view the attributes and insert data into `ephys.ClusteringParamSet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys.ClusteringParamSet.heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert clustering task manually\n",
    "params_ks = {\n",
    "    \"fs\": 30000,\n",
    "    \"fshigh\": 150,\n",
    "    \"minfr_goodchannels\": 0.1,\n",
    "    \"Th\": [10, 4],\n",
    "    \"lam\": 10,\n",
    "    \"AUCsplit\": 0.9,\n",
    "    \"minFR\": 0.02,\n",
    "    \"momentum\": [20, 400],\n",
    "    \"sigmaMask\": 30,\n",
    "    \"ThPr\": 8,\n",
    "    \"spkTh\": -6,\n",
    "    \"reorder\": 1,\n",
    "    \"nskip\": 25,\n",
    "    \"GPU\": 1,\n",
    "    \"Nfilt\": 1024,\n",
    "    \"nfilt_factor\": 4,\n",
    "    \"ntbuff\": 64,\n",
    "    \"whiteningRange\": 32,\n",
    "    \"nSkipCov\": 25,\n",
    "    \"scaleproc\": 200,\n",
    "    \"nPCs\": 3,\n",
    "    \"useRAM\": 0,\n",
    "}\n",
    "ephys.ClusteringParamSet.insert_new_params(\n",
    "    clustering_method=\"kilosort2\",\n",
    "    paramset_idx=0,\n",
    "    params=params_ks,\n",
    "    paramset_desc=\"Spike sorting using Kilosort2\",\n",
    ")\n",
    "ephys.ClusteringParamSet()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've inserted kilosort parameters into the `ClusteringParamSet` table,\n",
    "we're almost ready to sort our data. DataJoint uses a `ClusteringTask` table to\n",
    "manage which `EphysRecording` and `ClusteringParamSet` should be used during processing. \n",
    "\n",
    "This table is important for defining several important aspects of\n",
    "downstream processing. Let's view the attributes to get a better understanding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys.ClusteringTask.heading"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ClusteringTask` table contains two important attributes: \n",
    "+ `paramset_idx` \n",
    "+ `task_mode` \n",
    "\n",
    "The `paramset_idx` attribute tracks\n",
    "your kilosort parameter sets. You can choose the parameter set using which \n",
    "you want spike sort ephys data. For example, `paramset_idx=0` may contain\n",
    "default parameters for kilosort processing whereas `paramset_idx=1` contains your custom parameters for sorting. This\n",
    "attribute tells the `Processing` table which set of parameters you are processing in a given `populate()`.\n",
    "\n",
    "The `task_mode` attribute can be set to either `load` or `trigger`. When set to `load`,\n",
    "running the processing step initiates a search for exisiting kilosort output files. When set to `trigger`, the\n",
    "processing step will run kilosort on the raw data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys.ClusteringTask.insert1(\n",
    "    dict(\n",
    "        session_key,\n",
    "        insertion_number=1,\n",
    "        paramset_idx=0,\n",
    "        task_mode=\"load\",  # load or trigger\n",
    "        clustering_output_dir=\"processed/subject5/session1/probe_1/kilosort2-5_1\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys.Clustering.populate(session_key, display_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 4: Curate the Clustering Results (Optional)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While spike sorting is completed in the above step, you can optionally curate\n",
    "the output of image processing using the `Curation` table. For this demo, we\n",
    "will simply use the results of the spike sorting output from the `Clustering` task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys.Curation.heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_key = (ephys.ClusteringTask & session_key).fetch1(\"KEY\")\n",
    "ephys.Curation().create1_from_clustering_task(clustering_key)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the `Curation` table receives an entry, we can populate the remaining\n",
    "tables in the workflow including `CuratedClustering`, `WaveformSet`, and `LFP`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys.CuratedClustering.populate(session_key, display_progress=True)\n",
    "ephys.LFP.populate(session_key, display_progress=True)\n",
    "ephys.WaveformSet.populate(session_key, display_progress=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've populated the tables in this DataJoint pipeline, there are one of\n",
    "several next steps. If you have an existing pipeline for\n",
    "aligning waveforms to behavior data or other stimuli, you can easily\n",
    "invoke `element-event` or define your custom DataJoint tables to extend the\n",
    "pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 5: Visualization of Results**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will do some exploratory analysis by fetching the data from the database and creating a few plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfp_average = (ephys.LFP & \"insertion_number = '1'\").fetch1(\"lfp_mean\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the query above, we fetch a single `lfp_mean` attribute from the `LFP` table.\n",
    "We also restrict the query to insertion number 1.\n",
    "\n",
    "Let's go ahead and plot the LFP mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lfp_average)\n",
    "plt.title(\"Average LFP Waveform for Insertion 1\")\n",
    "plt.xlabel(\"Samples\")\n",
    "plt.ylabel(\"microvolts (uV)\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataJoint queries are a highly flexible tool to manipulate and visualize your data.\n",
    "After all, visualizing traces or generating rasters is likely just the start of\n",
    "your analysis workflow. This can also make the queries seem more complex at\n",
    "first. However, we'll walk through them slowly to simplify their content in this notebook. \n",
    "\n",
    "The examples below perform several operations using DataJoint queries:\n",
    "- Fetch the primary key attributes of all units that are in `insertion_number=1`.\n",
    "- Use **multiple restrictions** to fetch timestamps and create a raster plot.\n",
    "- Use a **join** operation and **multiple restrictions** to fetch a waveform\n",
    "  trace, along with unit data to create a single waveform plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_key = (ephys.ProbeInsertion & \"insertion_number = '1'\").fetch1(\"KEY\")\n",
    "units, unit_spiketimes = (\n",
    "    ephys.CuratedClustering.Unit\n",
    "    & insert_key\n",
    "    & 'unit IN (\"6\",\"7\",\"9\",\"14\",\"15\",\"17\",\"19\")'\n",
    ").fetch(\"unit\", \"spike_times\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.hstack(unit_spiketimes)\n",
    "y = np.hstack([np.full_like(s, u) for u, s in zip(units, unit_spiketimes)])\n",
    "plt.plot(x, y, \"|\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Unit\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_key = (ephys.CuratedClustering.Unit & insert_key & \"unit = '15'\").fetch1(\"KEY\")\n",
    "unit_data = (\n",
    "    ephys.CuratedClustering.Unit * ephys.WaveformSet.PeakWaveform & unit_key\n",
    ").fetch1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_rate = (ephys.EphysRecording & insert_key).fetch1(\n",
    "    \"sampling_rate\"\n",
    ") / 1000  # in kHz\n",
    "plt.plot(\n",
    "    np.r_[: unit_data[\"peak_electrode_waveform\"].size] * 1 / sampling_rate,\n",
    "    unit_data[\"peak_electrode_waveform\"],\n",
    ")\n",
    "plt.xlabel(\"Time (ms)\")\n",
    "plt.ylabel(r\"Voltage ($\\mu$V)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Following this tutorial, we've efficiently: \n",
    "- covered the essential functionality of `element-array-ephys`\n",
    "- acquired the skills to register the electrophysiology recordings for each probe in each experimental session\n",
    "- insert data into tables\n",
    "- execute the spike sort with Kilosort\n",
    "- visualize the results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Documentation and DataJoint tutorials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For detailed documentation on `element-array-ephys`:\n",
    "\n",
    "[`DataJoint Element for Extracellular Electrophysiology  - Documentation`](https://datajoint.com/docs/elements/element-array-ephys/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For detailed documentation and tutorials on general DataJoint principles that support collaboration, automation, reproducibility, and visualizations:\n",
    "\n",
    "[`DataJoint for Python - Interactive Tutorials`](https://github.com/datajoint/datajoint-tutorials) covers fundamentals, including table tiers, query operations, fetch operations, automated computations with the make function, and more.\n",
    "\n",
    "[`DataJoint for Python - Documentation`](https://datajoint.com/docs/core/datajoint-python/0.14/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run this tutorial on your own data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout this notebook, we've used DataJoint to work with database tables and keep\n",
    "data organized and automate analyses to increase efficiency of data processing. We've\n",
    "inserted data into tables, used queries to retrieve, manipulate, and visualize ephys data.\n",
    "\n",
    "Remember, this is just the beginning. As you grow familiar with DataJoint, you'll\n",
    "uncover even more ways to harness its capabilities for your specific research needs. \n",
    "\n",
    "\n",
    "To run this tutorial notebook on your own data, please use the following steps:\n",
    "- Download the mysql-docker image for DataJoint and run the container according to the\n",
    "  instructions provide in the repository.\n",
    "- Create a fork of this repository to your GitHub account.\n",
    "- Clone the repository and open the files using your IDE.\n",
    "- Add a code cell immediately after the first code cell in the notebook - we will setup\n",
    "  the local connection using this cell. In this cell, type in the following code. \n",
    "\n",
    "```python\n",
    "import datajoint as dj\n",
    "dj.config[\"database.host\"] = \"localhost\"\n",
    "dj.config[\"database.user\"] = \"<your-username>\"\n",
    "dj.config[\"database.password\"] = \"<your-password>\"\n",
    "dj.config[\"custom\"] = {\"imaging_root_data_dir\": \"path/to/your/data/dir\",\n",
    "\"database_prefix\": \"<your-username_>\"}\n",
    "dj.config.save_local()\n",
    "dj.conn()\n",
    "```\n",
    "\n",
    "- Run this code block above and proceed with the rest of the notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3p10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ff52d424e56dd643d8b2ec122f40a2e279e94970100b4e6430cb9025a65ba4cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
